{{- if .Values.loki.enabled }}
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: loki
  namespace: {{ .Values.global.namespace }}
  finalizers:
    - resources-finalizer.argocd.argoproj.io
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    argocd.argoproj.io/compare-options: "ServerSideDiff=true"
  labels:
    {{- include "tesk-standalone-stack.labels" . | nindent 4 }}
spec:
  project: {{ .Values.global.argoProject }}
  destination:
    namespace: {{ .Values.prometheus.namespace }}
    server: https://kubernetes.default.svc
  syncPolicy:
    managedNamespaceMetadata:
      labels:
        "admission.gatekeeper.sh/ignore": "pleaseignore"
    automated:
      enabled: true
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - ApplyOutOfSyncOnly=true
      - ServerSideApply=true
    retry:
      refresh: true
  source:
    repoURL: {{ .Values.loki.repoURL }}
    chart: {{ .Values.loki.chart }}
    targetRevision: {{ .Values.loki.chartVersion }}
    helm:
      valuesObject:
        loki:
          auth_enabled: false
          storage:
            use_thanos_objstore: false
            type: filesystem
            bucketNames:
              chunks: "thisisafakebucket" # Helm chart is now mad when this isn't set
              ruler: "thisisafakebucket"
              admin: "thisisafakebucket"
          commonStorageConfig:
            type: filesystem
          rulerConfig:
            storage:
              type: local
          commonConfig:
            replication_factor: 1
          schemaConfig:
            configs:
              - from: "2024-04-01"
                store: tsdb
                object_store: s3
                schema: v13
                index:
                  prefix: loki_index_
                  period: 24h
          ingester:
            chunk_encoding: snappy
          tracing:
            enabled: true
          querier:
            # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
            max_concurrent: 2

        deploymentMode: SingleBinary
        singleBinary:
          persistence:
            enableStatefulSetAutoDeletePVC: true
            storageClass: "{{ .Values.global.storage.defaultClass }}"
            enabled: true
            size: 5Gi
          replicas: 1
          resources:
            limits:
              cpu: 3
              memory: 4Gi
            requests:
              cpu: "0.2"
              memory: 100Mi
          extraEnv:
            # Keep a little bit lower than memory limits
            - name: GOMEMLIMIT
              value: 3750MiB

        chunksCache:
          # default is 500MB, with limited memory keep this smaller
          writebackSizeLimit: 10MB

        # Zero out replica counts of other deployment modes
        backend:
          replicas: 0
        read:
          replicas: 0
        write:
          replicas: 0

        ingester:
          replicas: 0
        querier:
          replicas: 0
        queryFrontend:
          replicas: 0
        queryScheduler:
          replicas: 0
        distributor:
          replicas: 0
        compactor:
          replicas: 0
        indexGateway:
          replicas: 0
        bloomCompactor:
          replicas: 0
        bloomGateway:
          replicas: 0
                              

{{- end }}